name: Dataset downloader and splitter With preprocessing V1.3
inputs:
  - {name: tensor_values_cdn, type: String, optional: false}
  - {name: masked_tensor_values_cdn, type: String, optional: false}
  - {name: test_size, type: Float, optional: true, default: "0.3"}
  - {name: val_split, type: Float, optional: true, default: "0.5"}
  - {name: random_state, type: Integer, optional: true, default: "42"}
  - {name: batch_size, type: Integer, optional: true, default: "32"}
  - {name: num_workers, type: Integer, optional: true, default: "0"}

outputs:
  - {name: tensor_values, type: Data}
  - {name: masked_tensor_values, type: Data}
  - {name: train_dataset, type: Data}
  - {name: val_dataset, type: Data}
  - {name: test_dataset, type: Data}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v1
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, tempfile, json, logging, gc
        import numpy as np
        import torch
        from torch.utils.data import TensorDataset, DataLoader
        import requests
        from requests.adapters import HTTPAdapter
        try:
            from urllib3.util import Retry
        except Exception:
            from urllib3 import Retry
        from sklearn.model_selection import train_test_split
        from sklearn.preprocessing import StandardScaler
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--tensor_values_cdn', required=True)
        parser.add_argument('--masked_tensor_values_cdn', required=True)
        parser.add_argument('--test_size', type=float, default=0.3)
        parser.add_argument('--val_split', type=float, default=0.5)
        parser.add_argument('--random_state', type=int, default=42)
        parser.add_argument('--batch_size', type=int, default=32)
        parser.add_argument('--num_workers', type=int, default=0)
        parser.add_argument('--tensor_values', required=True)
        parser.add_argument('--masked_tensor_values', required=True)
        parser.add_argument('--train_dataset', required=True)
        parser.add_argument('--val_dataset', required=True)
        parser.add_argument('--test_dataset', required=True)
        args = parser.parse_args()
        
        logging.basicConfig(stream=sys.stdout, level=logging.INFO)
        logger = logging.getLogger("pytorch_dataset_downloader_splitter")
        
        session = requests.Session()
        retry = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
        session.mount("http://", HTTPAdapter(max_retries=retry))
        session.mount("https://", HTTPAdapter(max_retries=retry))
        
        headers = {}
        
        
        def free_memory(*arrays):
            for arr in arrays:
                del arr
            gc.collect()
        
        
        def get_tensor_size_mb(tensor):
            return tensor.element_size() * tensor.nelement() / (1024 * 1024)
        
        
        def download_file(url):
            try:
                logger.info("Downloading from: %s", url)
                r = session.get(url, headers=headers, timeout=60, stream=True)
                r.raise_for_status()
                fd, tmp = tempfile.mkstemp()
                os.close(fd)
                with open(tmp, "wb") as f:
                    for chunk in r.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                logger.info("Successfully downloaded to: %s", tmp)
                return tmp
            except Exception as e:
                logger.error("Download failed for %s: %s", url, str(e))
                raise
        
        
        def load_data_multi_format(file_path):
            logger.info("Loading data from: %s", file_path)
        
            strategies = [
                ('PyTorch Tensor', _load_torch),
                ('NumPy (.npy)',   lambda p: np.load(p, allow_pickle=True)),
                ('NumPy (.npz)',   lambda p: np.load(p, allow_pickle=True)['data']),
                ('Parquet',        _load_parquet),
                ('CSV',            lambda p: _load_pandas(p, sep=',')),
                ('TSV',            lambda p: _load_pandas(p, sep='\t')),
                ('Excel',          _load_excel),
            ]
        
            last_error = None
            for name, fn in strategies:
                try:
                    logger.info("Trying: %s", name)
                    data = fn(file_path)
        
                    # Normalise to numpy float32 immediately
                    if isinstance(data, torch.Tensor):
                        data = data.numpy()
                    if not isinstance(data, np.ndarray):
                        data = np.array(data)
        
                    # Cast to float32 in-place to avoid a second full copy
                    if data.dtype != np.float32:
                        data = data.astype(np.float32, copy=False)
        
                    logger.info("✓ Loaded as %s – shape: %s", name, data.shape)
                    return data
                except Exception as e:
                    logger.debug("Failed (%s): %s", name, e)
                    last_error = e
        
            raise RuntimeError(f"Could not load file with any supported format. Last error: {last_error}")
        
        
        def _load_torch(path):
            t = torch.load(path, map_location='cpu')   # load ONCE
            return t.numpy() if isinstance(t, torch.Tensor) else t
        
        
        def _load_parquet(path):
            import pandas as pd
            file_size_mb = os.path.getsize(path) / (1024 * 1024)
            logger.info("Parquet file size: %.2f MB", file_size_mb)
            if file_size_mb < 500:
                return pd.read_parquet(path, engine="pyarrow").values
            # Chunked path for large files
            import pyarrow.parquet as pq
            chunks = []
            for batch in pq.ParquetFile(path).iter_batches(batch_size=100_000):
                chunks.append(batch.to_pandas().values)
            return np.vstack(chunks)
        
        
        def _load_pandas(path, sep=','):
            import pandas as pd
            return pd.read_csv(path, sep=sep).values
        
        
        def _load_excel(path):
            import pandas as pd
            return pd.read_excel(path).values
        
        
        def save_tensor_output(tensor, output_path, data_name):
            os.makedirs(output_path, exist_ok=True)
            save_path = os.path.join(output_path, f"{data_name}.pt")
            torch.save(tensor, save_path)
            logger.info("Saved %s to: %s", data_name, save_path)
            with open(output_path + ".meta.json", "w") as f:
                json.dump({"tensor_path": save_path,
                           "shape": list(tensor.shape),
                           "dtype": str(tensor.dtype)}, f)
            return save_path
        
        
        def save_pytorch_dataset(dataset_tensors, output_path, dataset_name):
            os.makedirs(output_path, exist_ok=True)
            save_path = os.path.join(output_path, f"{dataset_name}.pt")
            torch.save({'X': dataset_tensors[0], 'M': dataset_tensors[1]}, save_path)
            logger.info("Saved %s to: %s", dataset_name, save_path)
            with open(output_path + ".meta.json", "w") as f:
                json.dump({"dataset_path": save_path,
                           "shape_X": list(dataset_tensors[0].shape),
                           "shape_M": list(dataset_tensors[1].shape)}, f)
            return save_path
        
        
        try:
            # ============================================
            # 1. Download & Load – free raw files ASAP
            # ============================================
            logger.info("Downloading tensor values...")
            tensor_file = download_file(args.tensor_values_cdn)
            X = load_data_multi_format(tensor_file)          # float32 numpy array
            os.remove(tensor_file)
        
            logger.info("Downloading masked tensor values...")
            masked_file = download_file(args.masked_tensor_values_cdn)
            M = load_data_multi_format(masked_file)          # float32 numpy array
            os.remove(masked_file)
        
            logger.info("X shape: %s, M shape: %s", X.shape, M.shape)
        
            # ============================================
            # 2. Clean NaN / Inf  (in-place to avoid copy)
            # ============================================
            nan_count   = int(np.isnan(X).sum())
            posinf_count = int(np.isposinf(X).sum())
            neginf_count = int(np.isneginf(X).sum())
            logger.info("Found – NaN: %d, +Inf: %d, -Inf: %d",
                        nan_count, posinf_count, neginf_count)
        
            # np.nan_to_num with copy=False modifies in-place (avoids 8 GB duplicate)
            np.nan_to_num(X, copy=False, nan=0.0, posinf=0.0, neginf=0.0)
            logger.info("✓ Cleaned NaN / Inf in-place")
        
            # ============================================
            # 3. StandardScaler  (operates on 2-D view)
            # ============================================
            logger.info("Applying StandardScaler...")
            original_shape = X.shape
            n_samples = X.shape[0]
        
            # reshape() returns a view when possible – no memory copy
            X_2d = X.reshape(n_samples, -1)
        
            scaler = StandardScaler()
            # fit_transform returns float64 by default; cast to float16 straight away
            X_scaled_f16 = scaler.fit_transform(X_2d).astype(np.float16)
            del X_2d
        
            # Free the original float32 array now that scaling is done
            del X
            gc.collect()
        
            # Reshape back if needed (view when possible)
            if len(original_shape) > 2:
                X_scaled_f16 = X_scaled_f16.reshape(
                    (n_samples,) + original_shape[1:]
                )
        
            logger.info("✓ StandardScaler applied")
            logger.info("  Mean: %.6f, Std: %.6f",
                        X_scaled_f16.mean(), X_scaled_f16.std())
        
            # M only needs float16 cast
            M = M.astype(np.float16, copy=False)
        
            # ============================================
            # 4. Save full tensors
            #    Use torch.from_numpy() – zero-copy, shares memory with numpy array.
            #    Keep float16 (HalfTensor) to avoid doubling memory back to float32.
            # ============================================
            logger.info("Saving full tensor outputs...")
        
            X_tensor = torch.from_numpy(X_scaled_f16)   # HalfTensor, zero-copy
            M_tensor = torch.from_numpy(M)              # HalfTensor, zero-copy
        
            logger.info("X_tensor: %.2f MB", get_tensor_size_mb(X_tensor))
            logger.info("M_tensor: %.2f MB", get_tensor_size_mb(M_tensor))
        
            save_tensor_output(X_tensor, args.tensor_values, "tensor_values")
            save_tensor_output(M_tensor, args.masked_tensor_values, "masked_tensor_values")
            logger.info("✓ Saved full tensors")
        
            # ============================================
            # 5. Split data  (numpy splits are views/copies of float16 → small)
            # ============================================
            logger.info("Splitting data – test_size=%s, val_split=%s",
                        args.test_size, args.val_split)
        
            X_train, X_temp, M_train, M_temp = train_test_split(
                X_scaled_f16, M,
                test_size=args.test_size, random_state=args.random_state
            )
            # Free full arrays now that the train split is done
            del X_scaled_f16, M
            gc.collect()
        
            X_val, X_test, M_val, M_test = train_test_split(
                X_temp, M_temp,
                test_size=args.val_split, random_state=args.random_state
            )
            del X_temp, M_temp
            gc.collect()
        
            logger.info("Train: %s | Val: %s | Test: %s",
                        X_train.shape, X_val.shape, X_test.shape)
        
            # ============================================
            # 6. Convert splits to tensors, save, then free each split
            #    pin_memory=False – avoids an extra OS-level memory copy
            # ============================================
            logger.info("Saving split datasets...")
        
            # --- Train ---
            X_train_t = torch.from_numpy(X_train)
            M_train_t = torch.from_numpy(M_train)
            del X_train, M_train
            train_path = save_pytorch_dataset(
                (X_train_t, M_train_t), args.train_dataset, "train_dataset"
            )
            train_dataset = TensorDataset(X_train_t, M_train_t)
        
            # --- Val ---
            X_val_t = torch.from_numpy(X_val)
            M_val_t = torch.from_numpy(M_val)
            del X_val, M_val
            val_path = save_pytorch_dataset(
                (X_val_t, M_val_t), args.val_dataset, "val_dataset"
            )
            val_dataset = TensorDataset(X_val_t, M_val_t)
        
            # --- Test ---
            X_test_t = torch.from_numpy(X_test)
            M_test_t = torch.from_numpy(M_test)
            del X_test, M_test
            test_path = save_pytorch_dataset(
                (X_test_t, M_test_t), args.test_dataset, "test_dataset"
            )
            test_dataset = TensorDataset(X_test_t, M_test_t)
        
            gc.collect()
            logger.info("All datasets saved.")
            logger.info("Train: %s", train_path)
            logger.info("Val:   %s", val_path)
            logger.info("Test:  %s", test_path)
        
            # ============================================
            # 7. DataLoaders  (pin_memory=False saves RAM)
            # ============================================
            logger.info("Creating DataLoaders – batch_size=%s, num_workers=%s",
                        args.batch_size, args.num_workers)
        
            train_loader = DataLoader(train_dataset, batch_size=args.batch_size,
                                      shuffle=True,  num_workers=args.num_workers,
                                      pin_memory=False, drop_last=False)
            val_loader   = DataLoader(val_dataset,   batch_size=args.batch_size,
                                      shuffle=False, num_workers=args.num_workers,
                                      pin_memory=False, drop_last=False)
            test_loader  = DataLoader(test_dataset,  batch_size=args.batch_size,
                                      shuffle=False, num_workers=args.num_workers,
                                      pin_memory=False, drop_last=False)
        
            logger.info("✓ DataLoaders ready")
            logger.info("  Train batches: %d", len(train_loader))
            logger.info("  Val   batches: %d", len(val_loader))
            logger.info("  Test  batches: %d", len(test_loader))
        
            # ============================================
            # 8. Save summary JSON
            # ============================================
            dataloader_info = {
                "batch_size": args.batch_size,
                "num_workers": args.num_workers,
                "train_batches": len(train_loader),
                "val_batches": len(val_loader),
                "test_batches": len(test_loader),
                "train_samples": len(train_dataset),
                "val_samples": len(val_dataset),
                "test_samples": len(test_dataset),
                "full_tensor_shape": list(X_tensor.shape),
                "full_masked_tensor_shape": list(M_tensor.shape),
                "dtype": "float16",
                "preprocessing": {
                    "nan_count_removed": nan_count,
                    "posinf_count_removed": posinf_count,
                    "neginf_count_removed": neginf_count,
                    "scaler_applied": "StandardScaler",
                }
            }
            info_path = os.path.join(args.train_dataset, "dataloader_info.json")
            with open(info_path, "w") as f:
                json.dump(dataloader_info, f, indent=2)
            logger.info("DataLoader info saved to: %s", info_path)
        
        except Exception as e:
            logger.exception("Fatal error: %s", e)
            sys.exit(1)

    args:
      - --tensor_values_cdn
      - {inputValue: tensor_values_cdn}
      - --masked_tensor_values_cdn
      - {inputValue: masked_tensor_values_cdn}
      - --test_size
      - {inputValue: test_size}
      - --val_split
      - {inputValue: val_split}
      - --random_state
      - {inputValue: random_state}
      - --batch_size
      - {inputValue: batch_size}
      - --num_workers
      - {inputValue: num_workers}
      - --tensor_values
      - {outputPath: tensor_values}
      - --masked_tensor_values
      - {outputPath: masked_tensor_values}
      - --train_dataset
      - {outputPath: train_dataset}
      - --val_dataset
      - {outputPath: val_dataset}
      - --test_dataset
      - {outputPath: test_dataset}
