name: Evaluate Mask Aware Autoencoder v1
description: Evaluate Mask-Aware Autoencoder model on test dataset with masked metrics
inputs:
  - {name: trained_model, type: Model, description: "Trained model file (.pth)"}
  - {name: test_dataset, type: Data, description: "Test dataset (.pt file)"}
  - {name: batch_size, type: Integer, description: "Batch size for evaluation", default: "32"}
  - {name: num_workers, type: Integer, description: "Number of data loader workers", default: "0"}
  - {name: device, type: String, description: "Device to evaluate on (cuda, cpu, auto)", default: "auto"}
  - {name: loss_eps, type: Float, description: "Epsilon for numerical stability in loss", default: "0.000001"}
  - {name: compute_per_sample_metrics, type: String, description: "Compute per-sample statistics (true/false)", default: "true"}
  - {name: save_reconstructions, type: String, description: "Save reconstruction samples (true/false)", default: "true"}
  - {name: num_reconstruction_samples, type: Integer, description: "Number of samples to save for reconstruction", default: "100"}
  - {name: random_seed, type: Integer, description: "Random seed for reproducibility", default: "42"}

outputs:
  - {name: evaluation_results, type: Data, description: "Evaluation metrics and results"}
  - {name: reconstruction_samples, type: Data, description: "Reconstruction samples"}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import sys
        import json
        import traceback
        from pathlib import Path
        import numpy as np
        import torch
        import torch.nn as nn
        from torch.utils.data import DataLoader, TensorDataset
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
        
        parser = argparse.ArgumentParser(description="Evaluate Mask-Aware Autoencoder")
        parser.add_argument("--trained_model", type=str, required=True)
        parser.add_argument("--test_dataset", type=str, required=True)
        parser.add_argument("--batch_size", type=int, required=True)
        parser.add_argument("--num_workers", type=int, required=True)
        parser.add_argument("--device", type=str, required=True)
        parser.add_argument("--loss_eps", type=float, required=True)
        parser.add_argument("--compute_per_sample_metrics", type=str, required=True)
        parser.add_argument("--save_reconstructions", type=str, required=True)
        parser.add_argument("--num_reconstruction_samples", type=int, required=True)
        parser.add_argument("--random_seed", type=int, required=True)
        parser.add_argument("--evaluation_results", type=str, required=True)
        parser.add_argument("--reconstruction_samples", type=str, required=True)
        args = parser.parse_args()
        
        print("[evaluator] ============================================")
        print("[evaluator] Mask-Aware Autoencoder Evaluation")
        print("[evaluator] ============================================")
        
        def str_to_bool(s):
            return s.lower() in ['true', '1', 'yes']
        
        def get_device(device_str):
            if device_str == 'auto':
                return torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            return torch.device(device_str)
        
        torch.manual_seed(args.random_seed)
        np.random.seed(args.random_seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(args.random_seed)
        
        device = get_device(args.device)
        print(f"[evaluator] Device: {device}")
        print(f"[evaluator] Random seed: {args.random_seed}")
        
        def get_activation(name):
            activations = {
                'relu': nn.ReLU(),
                'tanh': nn.Tanh(),
                'elu': nn.ELU(),
                'leaky_relu': nn.LeakyReLU(),
                'sigmoid': nn.Sigmoid(),
                'none': None
            }
            return activations.get(name.lower(), nn.ReLU())
        
        class MAEEncoder(nn.Module):
            def __init__(self, feature_dim, latent_dim, layer1_units, layer2_units, activation, dropout, batch_norm):
                super(MAEEncoder, self).__init__()
                input_dim = 2 * feature_dim
                layers = []
                layers.append(nn.Linear(input_dim, layer1_units))
                if batch_norm:
                    layers.append(nn.BatchNorm1d(layer1_units))
                if activation is not None:
                    layers.append(activation)
                if dropout > 0:
                    layers.append(nn.Dropout(dropout))
                layers.append(nn.Linear(layer1_units, layer2_units))
                if batch_norm:
                    layers.append(nn.BatchNorm1d(layer2_units))
                if activation is not None:
                    layers.append(activation)
                if dropout > 0:
                    layers.append(nn.Dropout(dropout))
                layers.append(nn.Linear(layer2_units, latent_dim))
                self.encoder = nn.Sequential(*layers)
            
            def forward(self, x, m):
                x_in = torch.cat([x, m], dim=1)
                return self.encoder(x_in)
        
        class MAEDecoder(nn.Module):
            def __init__(self, feature_dim, latent_dim, layer1_units, layer2_units, activation, dropout, batch_norm):
                super(MAEDecoder, self).__init__()
                layers = []
                layers.append(nn.Linear(latent_dim, layer1_units))
                if batch_norm:
                    layers.append(nn.BatchNorm1d(layer1_units))
                if activation is not None:
                    layers.append(activation)
                if dropout > 0:
                    layers.append(nn.Dropout(dropout))
                layers.append(nn.Linear(layer1_units, layer2_units))
                if batch_norm:
                    layers.append(nn.BatchNorm1d(layer2_units))
                if activation is not None:
                    layers.append(activation)
                if dropout > 0:
                    layers.append(nn.Dropout(dropout))
                layers.append(nn.Linear(layer2_units, feature_dim))
                self.decoder = nn.Sequential(*layers)
            
            def forward(self, z):
                return self.decoder(z)
        
        class MaskAwareAutoencoder(nn.Module):
            def __init__(self, feature_dim, latent_dim, encoder_config, decoder_config):
                super(MaskAwareAutoencoder, self).__init__()
                self.feature_dim = feature_dim
                self.latent_dim = latent_dim
                self.encoder = MAEEncoder(feature_dim, latent_dim, encoder_config['layer1_units'], encoder_config['layer2_units'], encoder_config['activation'], encoder_config['dropout'], encoder_config['batch_norm'])
                self.decoder = MAEDecoder(feature_dim, latent_dim, decoder_config['layer1_units'], decoder_config['layer2_units'], decoder_config['activation'], decoder_config['dropout'], decoder_config['batch_norm'])
            
            def forward(self, x, m):
                z = self.encoder(x, m)
                x_hat = self.decoder(z)
                return x_hat, z
        
        def masked_mse(x_hat, x_true, mask, eps=1e-6):
            err = torch.square(x_hat - x_true) * mask
            num = torch.sum(err)
            den = torch.sum(mask)
            den = torch.clamp(den, min=eps)
            return (num / den).item()
        
        def masked_mae(x_hat, x_true, mask, eps=1e-6):
            err = torch.abs(x_hat - x_true) * mask
            num = torch.sum(err)
            den = torch.sum(mask)
            den = torch.clamp(den, min=eps)
            return (num / den).item()
        
        def masked_rmse(x_hat, x_true, mask, eps=1e-6):
            mse = masked_mse(x_hat, x_true, mask, eps)
            return np.sqrt(mse)
        
        def compute_masked_metrics(x_hat, x_true, mask, eps=1e-6):
            metrics = {}
            metrics['masked_mse'] = masked_mse(x_hat, x_true, mask, eps)
            metrics['masked_mae'] = masked_mae(x_hat, x_true, mask, eps)
            metrics['masked_rmse'] = masked_rmse(x_hat, x_true, mask, eps)
            return metrics
        
        def compute_per_sample_metrics(x_hat, x_true, mask, eps=1e-6):
            err_squared = torch.square(x_hat - x_true) * mask
            err_abs = torch.abs(x_hat - x_true) * mask
            num_mse = torch.sum(err_squared, dim=1)
            num_mae = torch.sum(err_abs, dim=1)
            den = torch.sum(mask, dim=1)
            den = torch.clamp(den, min=eps)
            per_sample_mse = (num_mse / den).cpu().numpy()
            per_sample_mae = (num_mae / den).cpu().numpy()
            per_sample_rmse = np.sqrt(per_sample_mse)
            
            stats = {
                'mse': {
                    'mean': float(np.mean(per_sample_mse)),
                    'std': float(np.std(per_sample_mse)),
                    'min': float(np.min(per_sample_mse)),
                    'max': float(np.max(per_sample_mse)),
                    'median': float(np.median(per_sample_mse))
                },
                'mae': {
                    'mean': float(np.mean(per_sample_mae)),
                    'std': float(np.std(per_sample_mae)),
                    'min': float(np.min(per_sample_mae)),
                    'max': float(np.max(per_sample_mae)),
                    'median': float(np.median(per_sample_mae))
                },
                'rmse': {
                    'mean': float(np.mean(per_sample_rmse)),
                    'std': float(np.std(per_sample_rmse)),
                    'min': float(np.min(per_sample_rmse)),
                    'max': float(np.max(per_sample_rmse)),
                    'median': float(np.median(per_sample_rmse))
                }
            }
            return stats
        
        try:
            print(f"[evaluator] Loading trained model from: {args.trained_model}")
            model_files = list(Path(args.trained_model).glob("*.pth"))
            if not model_files:
                raise FileNotFoundError(f"No .pth file found in {args.trained_model}")
            model_path = model_files[0]
            print(f"[evaluator] Model file: {model_path}")
            checkpoint = torch.load(model_path, map_location=device)
            config = checkpoint['model_config']
            print(f"[evaluator] Model config loaded")
            print(f"[evaluator]   Feature dim: {config['feature_dim']}")
            print(f"[evaluator]   Latent dim: {config['latent_dim']}")
            
            encoder_config = {
                'layer1_units': config['encoder']['layer1_units'],
                'layer2_units': config['encoder']['layer2_units'],
                'activation': get_activation(config['encoder']['activation']),
                'dropout': config['encoder']['dropout'],
                'batch_norm': str_to_bool(config['encoder']['batch_norm'])
            }
            decoder_config = {
                'layer1_units': config['decoder']['layer1_units'],
                'layer2_units': config['decoder']['layer2_units'],
                'activation': get_activation(config['decoder']['activation']),
                'dropout': config['decoder']['dropout'],
                'batch_norm': str_to_bool(config['decoder']['batch_norm'])
            }
            
            model = MaskAwareAutoencoder(feature_dim=config['feature_dim'], latent_dim=config['latent_dim'], encoder_config=encoder_config, decoder_config=decoder_config)
            model.load_state_dict(checkpoint['model_state_dict'])
            model = model.to(device)
            model.eval()
            print(f"[evaluator] Model loaded and moved to {device}")
            print(f"[evaluator] Total parameters: {sum(p.numel() for p in model.parameters()):,}")
        except Exception as e:
            print(f"[evaluator] ERROR: Failed to load model: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        try:
            print(f"[evaluator] Loading test dataset...")
            test_files = list(Path(args.test_dataset).glob("*.pt"))
            if not test_files:
                raise FileNotFoundError(f"No .pt file found in {args.test_dataset}")
            test_data = torch.load(test_files[0])
            test_dataset = TensorDataset(test_data['X'], test_data['M'])
            print(f"[evaluator] Test samples: {len(test_dataset)}")
            
            test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers, pin_memory=(device.type == 'cuda'), drop_last=False)
            print(f"[evaluator] Test batches: {len(test_loader)}")
        except Exception as e:
            print(f"[evaluator] ERROR: Failed to load test dataset: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        try:
            print(f"[evaluator] ============================================")
            print(f"[evaluator] Starting Evaluation")
            print(f"[evaluator] ============================================")
            
            all_x_true = []
            all_x_hat = []
            all_masks = []
            all_latents = []
            batch_metrics = []
            
            with torch.no_grad():
                for batch_idx, (xb, mb) in enumerate(test_loader):
                    xb = xb.to(device)
                    mb = mb.to(device)
                    x_hat, z = model(xb, mb)
                    batch_metric = compute_masked_metrics(x_hat, xb, mb, args.loss_eps)
                    batch_metrics.append(batch_metric)
                    all_x_true.append(xb.cpu())
                    all_x_hat.append(x_hat.cpu())
                    all_masks.append(mb.cpu())
                    all_latents.append(z.cpu())
                    
                    if (batch_idx + 1) % 10 == 0:
                        print(f"[evaluator] Processed {batch_idx + 1}/{len(test_loader)} batches")
            
            all_x_true = torch.cat(all_x_true, dim=0)
            all_x_hat = torch.cat(all_x_hat, dim=0)
            all_masks = torch.cat(all_masks, dim=0)
            all_latents = torch.cat(all_latents, dim=0)
            
            print(f"[evaluator] Evaluation complete. Processed {len(all_x_true)} samples")
            
            overall_metrics = compute_masked_metrics(all_x_hat, all_x_true, all_masks, args.loss_eps)
            
            print(f"[evaluator] ============================================")
            print(f"[evaluator] Overall Masked Metrics")
            print(f"[evaluator] ============================================")
            print(f"[evaluator] Masked MSE:  {overall_metrics['masked_mse']:.6f}")
            print(f"[evaluator] Masked MAE:  {overall_metrics['masked_mae']:.6f}")
            print(f"[evaluator] Masked RMSE: {overall_metrics['masked_rmse']:.6f}")
            print(f"[evaluator] ============================================")
            
            per_sample_stats = None
            if str_to_bool(args.compute_per_sample_metrics):
                print(f"[evaluator] Computing per-sample statistics...")
                per_sample_stats = compute_per_sample_metrics(all_x_true, all_x_hat, all_masks, args.loss_eps)
                print(f"[evaluator] ============================================")
                print(f"[evaluator] Per-Sample Statistics")
                print(f"[evaluator] ============================================")
                print(f"[evaluator] MSE  - Mean: {per_sample_stats['mse']['mean']:.6f}, Std: {per_sample_stats['mse']['std']:.6f}")
                print(f"[evaluator] MAE  - Mean: {per_sample_stats['mae']['mean']:.6f}, Std: {per_sample_stats['mae']['std']:.6f}")
                print(f"[evaluator] RMSE - Mean: {per_sample_stats['rmse']['mean']:.6f}, Std: {per_sample_stats['rmse']['std']:.6f}")
                print(f"[evaluator] ============================================")
        
        except Exception as e:
            print(f"[evaluator] ERROR: Evaluation failed: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        try:
            print(f"[evaluator] Saving evaluation results...")
            os.makedirs(args.evaluation_results, exist_ok=True)
            results_path = os.path.join(args.evaluation_results, 'evaluation_results.json')
            
            results = {
                'overall_metrics': overall_metrics,
                'per_sample_statistics': per_sample_stats,
                'test_samples': len(all_x_true),
                'model_info': {
                    'feature_dim': config['feature_dim'],
                    'latent_dim': config['latent_dim'],
                    'trained_epoch': int(checkpoint.get('epoch', 0)),
                    'best_val_loss': float(checkpoint.get('best_val_loss', 0.0))
                },
                'evaluation_config': {
                    'batch_size': args.batch_size,
                    'loss_eps': args.loss_eps,
                    'device': str(device)
                }
            }
            
            with open(results_path, 'w') as f:
                json.dump(results, f, indent=2)
            print(f"[evaluator] Evaluation results saved: {results_path}")
        except Exception as e:
            print(f"[evaluator] ERROR: Failed to save evaluation results: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        try:
            if str_to_bool(args.save_reconstructions):
                print(f"[evaluator] Saving reconstruction samples...")
                os.makedirs(args.reconstruction_samples, exist_ok=True)
                num_samples = min(args.num_reconstruction_samples, len(all_x_true))
                samples_path = os.path.join(args.reconstruction_samples, 'reconstruction_samples.pt')
                
                torch.save({
                    'x_true': all_x_true[:num_samples],
                    'x_reconstructed': all_x_hat[:num_samples],
                    'masks': all_masks[:num_samples],
                    'latents': all_latents[:num_samples]
                }, samples_path)
                print(f"[evaluator] Saved {num_samples} reconstruction samples: {samples_path}")
        except Exception as e:
            print(f"[evaluator] ERROR: Failed to save reconstruction samples: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        print(f"[evaluator] ============================================")
        print(f"[evaluator] Evaluation Complete!")
        print(f"[evaluator] ============================================")
        print(f"[evaluator] Results: {results_path}")
        print(f"[evaluator] Masked MSE: {overall_metrics['masked_mse']:.6f}")
        print(f"[evaluator] Masked MAE: {overall_metrics['masked_mae']:.6f}")
        print(f"[evaluator] Masked RMSE: {overall_metrics['masked_rmse']:.6f}")
        print(f"[evaluator] ============================================")

    args:
      - --trained_model
      - {inputPath: trained_model}
      - --test_dataset
      - {inputPath: test_dataset}
      - --batch_size
      - {inputValue: batch_size}
      - --num_workers
      - {inputValue: num_workers}
      - --device
      - {inputValue: device}
      - --loss_eps
      - {inputValue: loss_eps}
      - --compute_per_sample_metrics
      - {inputValue: compute_per_sample_metrics}
      - --save_reconstructions
      - {inputValue: save_reconstructions}
      - --num_reconstruction_samples
      - {inputValue: num_reconstruction_samples}
      - --random_seed
      - {inputValue: random_seed}
      - --evaluation_results
      - {outputPath: evaluation_results}
      - --reconstruction_samples
      - {outputPath: reconstruction_samples}
